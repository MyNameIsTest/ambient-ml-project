{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/src\n",
      "LICENSE    deprecated\t   music-transformer-train.ipynb  train.py\n",
      "README.md  dist_train.py   params.py\t\t\t  utils.py\n",
      "custom\t   generate.py\t   preprocess.py\n",
      "data.py    midi_processor  readme_src\n",
      "dataset    model.py\t   requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%cd /src\n",
    "!ls\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class Data has \"329\" files>\n"
     ]
    }
   ],
   "source": [
    "from model import MusicTransformerDecoder\n",
    "from custom.layers import *\n",
    "from custom import callback\n",
    "import params as par\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from data import Data\n",
    "import utils\n",
    "import argparse\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "tf.executing_eagerly()\n",
    "\n",
    "# set arguments\n",
    "l_r = 0.0001\n",
    "batch_size = 1\n",
    "pickle_dir = '/pickle'\n",
    "max_seq = 256\n",
    "epochs = 1\n",
    "is_reuse = False\n",
    "load_path = None\n",
    "save_path = './save'\n",
    "multi_gpu = True\n",
    "num_layer = 6\n",
    "\n",
    "\n",
    "# load data\n",
    "dataset = Data('/data/classic_piano_preprocessed')\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# load model\n",
    "learning_rate = callback.CustomSchedule(par.embedding_dim) if l_r is None else l_r\n",
    "opt = Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "# define model\n",
    "mt = MusicTransformerDecoder(\n",
    "            embedding_dim=256,\n",
    "            vocab_size=par.vocab_size,\n",
    "            num_layer=num_layer,\n",
    "            max_seq=max_seq,\n",
    "            dropout=0.2,\n",
    "            debug=False, loader_path=load_path)\n",
    "mt.compile(optimizer=opt, loss=callback.transformer_dist_train_loss)\n",
    "\n",
    "\n",
    "# define tensorboard writer\n",
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "train_log_dir = 'logs/mt_decoder/'+current_time+'/train'\n",
    "eval_log_dir = 'logs/mt_decoder/'+current_time+'/eval'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "eval_summary_writer = tf.summary.create_file_writer(eval_log_dir)\n",
    "\n",
    "\n",
    "# Train Start\n",
    "idx = 0\n",
    "for e in range(epochs):\n",
    "    mt.reset_metrics()\n",
    "    for b in range(len(dataset.files) // batch_size):\n",
    "        try:\n",
    "            batch_x, batch_y = dataset.slide_seq2seq_batch(batch_size, max_seq)\n",
    "        except:\n",
    "            continue\n",
    "        result_metrics = mt.train_on_batch(batch_x, batch_y)\n",
    "        if b % 100 == 0:\n",
    "            eval_x, eval_y = dataset.slide_seq2seq_batch(batch_size, max_seq, 'eval')\n",
    "            eval_result_metrics, weights = mt.evaluate(eval_x, eval_y)\n",
    "            mt.save(save_path)\n",
    "            with train_summary_writer.as_default():\n",
    "                if b == 0:\n",
    "                    tf.summary.histogram(\"target_analysis\", batch_y, step=e)\n",
    "                    tf.summary.histogram(\"source_analysis\", batch_x, step=e)\n",
    "\n",
    "                tf.summary.scalar('loss', result_metrics[0], step=idx)\n",
    "                tf.summary.scalar('accuracy', result_metrics[1], step=idx)\n",
    "\n",
    "            with eval_summary_writer.as_default():\n",
    "                if b == 0:\n",
    "                    mt.sanity_check(eval_x, eval_y, step=e)\n",
    "\n",
    "                tf.summary.scalar('loss', eval_result_metrics[0], step=idx)\n",
    "                tf.summary.scalar('accuracy', eval_result_metrics[1], step=idx)\n",
    "                for i, weight in enumerate(weights):\n",
    "                    with tf.name_scope(\"layer_%d\" % i):\n",
    "                        with tf.name_scope(\"w\"):\n",
    "                            utils.attention_image_summary(weight, step=idx)\n",
    "                # for i, weight in enumerate(weights):\n",
    "                #     with tf.name_scope(\"layer_%d\" % i):\n",
    "                #         with tf.name_scope(\"_w0\"):\n",
    "                #             utils.attention_image_summary(weight[0])\n",
    "                #         with tf.name_scope(\"_w1\"):\n",
    "                #             utils.attention_image_summary(weight[1])\n",
    "            idx += 1\n",
    "            print('\\n====================================================')\n",
    "            print('Epoch/Batch: {}/{}'.format(e, b))\n",
    "            print('Train >>>> Loss: {:6.6}, Accuracy: {}'.format(result_metrics[0], result_metrics[1]))\n",
    "            print('Eval >>>> Loss: {:6.6}, Accuracy: {}'.format(eval_result_metrics[0], eval_result_metrics[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
